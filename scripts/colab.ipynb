{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install glfw\n",
    "!pip install gtimer\n",
    "!git clone -b colab-mve https://github.com/kin-7777777/rlkit.git\n",
    "%cd /content/rlkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install free-mujoco-py\n",
    "!pip install -q dm_control>=1.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym.envs.mujoco import HalfCheetahEnv\n",
    "from gym.envs.mujoco.half_cheetah_v3 import HalfCheetahEnv\n",
    "from gym.envs.classic_control import pendulum\n",
    "\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.data_management.env_replay_buffer import EnvReplayBuffer\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.launchers.launcher_util import setup_logger\n",
    "from rlkit.samplers.data_collector import MdpPathCollector\n",
    "from rlkit.torch.sac.policies import TanhGaussianPolicy, MakeDeterministic\n",
    "from rlkit.torch.sac.sac_g import SACGTrainer\n",
    "from rlkit.torch.networks import ConcatMlp\n",
    "from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm\n",
    "\n",
    "from rlkit.envs.HM_arena_continuous_task1_max_speed_01_env import HM_arena_continuous_task1_max_speed_01Env\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from gamma.flows import (\n",
    "    make_conditional_flow,\n",
    ")\n",
    "from gamma.td.distributions import BootstrapTarget\n",
    "from gamma.td.structs import (\n",
    "    ReplayPool,\n",
    "    Policy,\n",
    ")\n",
    "from gamma.utils import (\n",
    "    mkdir,\n",
    "    set_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(variant):\n",
    "    expl_env = NormalizedBoxEnv(HM_arena_continuous_task1_max_speed_01Env())\n",
    "    eval_env = NormalizedBoxEnv(HM_arena_continuous_task1_max_speed_01Env())\n",
    "    obs_dim = expl_env.observation_space.low.size\n",
    "    action_dim = eval_env.action_space.low.size\n",
    "    \n",
    "    ## set seed\n",
    "    seed = variant['seed']\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    eval_env.seed(seed)\n",
    "    expl_env.seed(seed)\n",
    "    \n",
    "    condition_dims = {\n",
    "        's': obs_dim,\n",
    "        'a': action_dim,\n",
    "    }\n",
    "\n",
    "    M = variant['layer_size']\n",
    "    qf1 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    qf2 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    eval_path_collector = MdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = MdpPathCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = EnvReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        expl_env,\n",
    "    )\n",
    "    ## initialize conditional spline flow\n",
    "    g_model = make_conditional_flow(obs_dim, [M, M], condition_dims)\n",
    "    \n",
    "    ## target model is analogous to a target Q-function\n",
    "    g_target_model = copy.deepcopy(g_model)\n",
    "    \n",
    "    ## bootstrapped target distribution is mixture of\n",
    "    ## single-step gaussian (with weight `1 - discount`)\n",
    "    ## and target model (with weight `discount`)\n",
    "    g_bootstrap = BootstrapTarget(g_target_model, variant['trainer_kwargs'][\"g_discount\"])\n",
    "    \n",
    "    trainer = SACGTrainer(\n",
    "        env=eval_env,\n",
    "        policy=policy,\n",
    "        qf1=qf1,\n",
    "        qf2=qf2,\n",
    "        g_model=g_model,\n",
    "        g_target_model=g_target_model,\n",
    "        g_bootstrap=g_bootstrap,\n",
    "        **variant['trainer_kwargs']\n",
    "    )\n",
    "    algorithm = TorchBatchRLAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        **variant['algorithm_kwargs']\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # noinspection PyTypeChecker\n",
    "    variant = dict(\n",
    "        algorithm=\"SACG\",\n",
    "        version=\"normal\",\n",
    "        layer_size=256,\n",
    "        replay_buffer_size=int(1E6),\n",
    "        seed=0,\n",
    "        algorithm_kwargs=dict(\n",
    "            # num_epochs=3000,\n",
    "            num_epochs=300,\n",
    "            num_eval_steps_per_epoch=5000,\n",
    "            num_trains_per_train_loop=1000,\n",
    "            num_expl_steps_per_train_loop=1000,\n",
    "            min_num_steps_before_training=1000,\n",
    "            max_path_length=1000,\n",
    "            batch_size=256,\n",
    "        ),\n",
    "        trainer_kwargs=dict(\n",
    "            policy_lr=1E-4,\n",
    "            qf_lr=1E-4,\n",
    "            reward_scale=1,\n",
    "            use_automatic_entropy_tuning=False,\n",
    "            g_discount=0.80,\n",
    "            # g_sample_discount=0.90,\n",
    "            g_lr=1E-4,\n",
    "            g_tau = 0.005,\n",
    "            g_sigma=0.01,\n",
    "            g_mve_discount=0.99,\n",
    "            g_mve_horizon=3,\n",
    "        ),\n",
    "    )\n",
    "    setup_logger('gamma-mve-0.01speed', variant=variant)\n",
    "    ptu.set_gpu_mode(True)  # optionally set the GPU (default=False)\n",
    "    set_device('cuda:0') # 'cpu' or 'cuda:0' for gamma model device\n",
    "    experiment(variant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mjrl-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3002e2bd1009c04c7587a9239c893919c77d3c69923abf5026b12c96c5ba560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
